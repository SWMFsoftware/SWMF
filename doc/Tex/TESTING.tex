%  Copyright (C) 2002 Regents of the University of Michigan, portions used with permission 
%  For more information, see http://csem.engin.umich.edu/tools/swmf
\documentclass[twoside,10pt]{article}

\title{Testing Procedures for the Space Weather Modeling Framework\\
       \hfill \\
       \SWMFLOGOVERSION}

\author{G\'abor T\'oth\\
  {\it Center for Space Environment Modeling}\\
  {\it The University of Michigan}}

\input HEADER

\section{Description of the Testing Philosophy}

The Space Weather Modeling Framework (SWMF) is
composed of the core of the framework and the various components
modeling the physics domains. Since the components are developed 
independently, it is neither possible nor desirable to enforce a
rigorous and comprehensive testing procedure for each component.
As a minimum, however, we require that new components are provided
with at least one functionality test before they can be integrated
into the SWMF.

On the other hand we must ensure that the core of the SWMF and
the key components developed at the Center for Space Environment
Modeling (CSEM) are well tested and reliable. 
We also established some base-line testing for the whole
framework involving all the components to verify that the SWMF
works as expected for a more or less typical space weather simulation
run. This test can be run on different platforms, and it serves as
the portability testing for the SWMF.

In summary our testing philosophy follows a layered approach:
\begin{itemize}
\item The SWMF core: individual unit testing
\item Key components: extensive functionality test suite
\item New components: at least one functionality test
\item The SWMF as a whole: comprehensive portability tests
\end{itemize}
The core of the SWMF is tested by unit testing, by the functionality
test suite and also by the portability tests. The key components
are tested by the functionality test suite and the portability tests.
New components are tested by their functionality test and the 
portability tests. Finally all components and couplers are tested
by the portability tests.

\section{Description of Unit Testing}

The core of the SWMF consists of two layers:
\begin{itemize}
\item The super structure:  CON/Control, CON/Interface, CON/Stubs
\item The infra structure: CON/Library, CON/Coupler, share/, util/
\end{itemize}
The super structure, as the name suggests, can only be tested
together with the components and the infra structure. 
This complexity can be somewhat reduced if the components are replaced
with {\it stubs}. The stub components do not do any computation,
they simply advance the simulation time and use up some CPU time.
The stub components are implemented in the CON/Stubs directory.

\subsection{Testing CON/Control with CON/Stubs}

The SWMF can be compiled with the stub components if the 
\begin{verbatim}
INT_VERSION = Stubs
\end{verbatim}
is selected in {\tt Makefile.def}. All component versions
can be set to 'Empty'. With this choice the core
of the SWMF can be tested without running the components.
An example parameter and an example layout file is provided in
\begin{verbatim}
Param/PARAM.in.test.stubs
Param/LAYOUT.in.test.stubs
\end{verbatim}
To run the test, compile the SWMF with INT\_VERSION = Stubs
and empty component version then
\begin{verbatim}
make rundir
cd run
cp Param/PARAM.in.test.stubs PARAM.in
cp Param/LAYOUT.in.test.stubs LAYOUT.in
mpirun -np 2 SWMF.exe
\end{verbatim}
This test is intended for developers only, so the output is
relatively complicated. Other than printing to the screen,
the test creates several files
\begin{verbatim}
cd run
ls STDOUT/*.log ??restart_*
\end{verbatim}
The stub components can also be used to predict the parallel 
execution time for various layouts and control parameters.
An alternative approach is to use the Scripts/Performance.pl script.

\subsection{Testing CON/Library}

There is a test for the registration of components.
The list and layout of registered components is described by the 
LAYOUT.in file. An example file is provided in CON/Library/src.
To test the reading of this file and the various functions provided
by CON\_world, CON\_comp\_info and CON\_comp\_param, run 
\begin{verbatim}
cd CON/Library/src
make test
\end{verbatim}
This test is intended for developers only, 
so the output is relatively complicated.
One can change the LAYOUT.in file or the number of processors to
do more extensive testing.

The other modules in this directory (CON\_time and CON\_physics) are
relatively simple and they do not have a unit tester. These modules
are tested in the functionality and portability tests.

\subsection{Testing CON/Interface}

The CON/Interface directory contains the couplers between the 
components of the SWMF. These couplers cannot be tested by themselves.
The interfaces are tested by the portability tests 
(see section~\ref{sec:portability}).

\subsection{Testing CON/Coupler}

The CON/Coupler directory contains the parallel coupling toolkit of the SWMF.
This toolkit is used in some of the component couplers. 
The unit tester for the coupling toolkit is CON\_test\_global\_message\_pass.
This module has been used in the past to test the SWMF coupling toolkit. 
To avoid various problems with the compilers on the SGI Altix machines, 
the unit tester has been removed recently. 
The coupling toolkit is tested by the portability tests
(see section~\ref{sec:portability}).


\subsection{Testing share/Library}

This library is used by the SWMF as well as the stand alone components.
It is crucial to thoroughly test all methods provided by this library.
To run the unit tests
\begin{verbatim}
cd share/Library/test
make tests
\end{verbatim}
Although the output looks rather complex, it is mostly caused by the
compiler messages and the verbose information provided by the make
program. To get a cleaner output, rerun the tests as
\begin{verbatim}
make -s tests
\end{verbatim}
There should be now very limited output reporting the tests
of the various modules and methods. Some of the tests may show small
differences relative to the expected results due to round off errors.


\subsection{Testing util/TIMING and util/NOMPI}

The TIMING utility provides a simple and compiler independent utility
to measure the CPU time spent on various parts of a Fortran code.
The NOMPI utility allows to compile an MPI parallel F90 code with the
NOMPI library instead of the MPI library. The resulting executable
can run on a single processor. The NOMPI utility is useful for debugging 
purposes.

The TIMING can be tested with
\begin{verbatim}
cd util/Library/src
make tests
\end{verbatim}
An example output is provided in 'tests.log'. To compare with this,
rerun the tests like this
\begin{verbatim}
make -s tests > tmp.log
diff tmp.log tests.log
\end{verbatim}
Note that the timings and the order of the output can vary from
test run to test run. Two of the four tests involve the NOMPI
library, although only a few of the NOMPI methods are used.

\section{Description of Functionality Testing}

The functionality test aims at testing the functionality of 
a complex software as a whole. It should be comprehensive enough
to cover most of the functionality of the software. On the
other hand it should run fast on limited computer resources,
since the main use of the functionality test is during development.

Ideally each component and the whole SWMF should have an extensive
functionality test suite. 
Unfortunately most of the science codes are not developed with 
functionality testing in mind, and it would be impractical to demand
a rigorous test suite for all the components. In the past years
our experience showed that one should require at least one short
functionality test for a new component before it can be integrated into
the SWMF. Without such a test it is practically impossible to guarantee
that the integration has not changed the behavior of the component.

Repeated execution of the functionality test and a
comparison with earlier test results is vital during code development.
This procedure checks that the changes do not introduce unexpected side 
effects. Running the functionality test on various
platforms serves as a partial portability test. Since the 
functionality tests can be timed, they can also be used to compare
the execution speed on different platforms.

\subsection{Functionality Test Suite for the BATS-R-US code}

The most complex and crucial component in the SWMF is the 
Block Adaptive Tree Solarwind Roe Upwind Scheme (BATS-R-US) code
which is a parallel magnetohydrodynamics (MHD) code which uses a
block adaptive grid and sophisticated spatial and temporal discretization.
The BATS-R-US code is used as the Solar Corona (SC), the Inner Heliosphere (IH)
and the Global Magnetosphere (GM) components in the SWMF.

The BATS-R-US code has hundreds of adjustable parameters that interact
in non-trivial ways. We have developed a test suite
which tests the BATS-R-US code and its interaction with the IE/Ridley\_serial
ionosphere electrodynamics model. This test suite can be run in
the stand alone BATS-R-US code (except for the tests involving the
IE component) as well as in the SWMF. Here we describe how the
test suite can be run in the SWMF.

The test suite resides in
\begin{verbatim}
  GM/BATSRUS/Param/TESTSUITE
\end{verbatim}
The functionality test suite requires two components only,
so the following configuration is recommended:
\begin{verbatim}
  Config.pl -v=Empty,GM/BATSRUS,IE/Ridley_serial
  Config.pl -g=GM:8,8,8,400,100 -double
  make
  make PIDL
  make PSPH
  make rundir
\end{verbatim}
After compilation and creation of the run directory, the test suite
can be executed on 2 processors with
\begin{verbatim}
  Scripts/TestSuite func
\end{verbatim}
The results will be stored in the run directory
\begin{verbatim}
  run/test.000
  run/test.001
  ...
  run/test.035
\end{verbatim}
An individual test can be run with the {\tt TestSWMF} script,
for example the first test of the test suite is 
\begin{verbatim}
  Scripts/TestSWMF -Limiter=beta -Plottype=idltecamr
\end{verbatim}
Two test suite results obtained with different versions of the code,
or with different platforms/compilers, can be compared for 
consistency and performance:
\begin{verbatim}
  Scripts/TestCompare run1 run2
  Scripts/TestCompare -speed run1 run2
\end{verbatim}
The configuration of BATS-R-US to a 'covariant' (generalized coordinate) version
can be tested with
\begin{verbatim}
  Scripts/TestCovariant
\end{verbatim}
All these scripts provide a complete usage information when called with the 
{\tt -h} flag.

\section{Description of Portability Testing \label{sec:portability}}

The following tests involve all or most of the components of the SWMF.
They test SWMF as it is used in actual space weather modeling applications.
By running these tests on various platforms, the portability of SWMF
can be tested. The test can also serve as a benchmark for parallel performance.
Finally the input parameters and layout files can be used as a starting
point for production runs.

The tests can be performed on any of the target platforms 
(SGI Altix, SGI Origin 3000, Compaq ES45, Linux cluster) without any change
as long as there is a sufficient number of CPU-s available.
The only platform dependent step is the installation of the SWMF
which is described in the user manual. 

\subsection{The 8 Component Test}

This test demonstrates faster than real time execution of the
SWMF with all 8 components. The test was built up in six stages,
the last stage constitutes the actual performance test.
\begin{enumerate}
\item Create a steady state SC solution.
\item Create a steady state IH solution coupled to the SC.
\item Run SC, IH and SP for 4 hours real time with 
      a CME initiated in SC using an Eruptive Event generator.
\item Create a start up GM solution based on the IH solution.
\item Create a start up GM+IM+IE+UA solution.
\item Run all 8 components in time accurate mode restarting from the
      SC+IH and GM+IM+IE+UA solutions. 
      The SP and RB components start from scratch.
\end{enumerate}
We provide the parameter and layout files for all six stages,
but here we only describe how the final stage can be run using
the restart files saved from the previous stages.
This takes much less time than running all stages, 
and it tests the whole SWMF. The README file in the SWMF\_TEST
tar ball contains a detailed description of all stages.

\subsubsection{Configuration and Compilation}

For sake of simplicity one can configure SWMF with all 8 components
(no Empty versions) so that all 6 stages can be done with the same
executable. 
\begin{verbatim}
  Config.pl -v=SC/BATSRUS,IH/BATSRUS_share,GM/BATSRUS,SP/Kota,IM/RCM
  Config.pl -v=IE/Ridley_serial,UA/GITM,RB/RiceV5
\end{verbatim}
This is the default configuration except for the SC
component, which has the Empty version by default.
The source code for the SC/BATSRUS component is configured 
from the GM/BATSRUS source code and the subroutines, functions and 
modules are renamed to avoid name conflicts. This automated code
generation takes some time.
Note that one can select the IH/BATSRUS version instead of IH/BATSRUS\_share.
In that case the source code for the IH/BATSRUS version is generated 
from the GM/BATSRUS source code with a similar renaming procedure.

The grid sizes of the components should be set to be sufficient for the test
for a given processor layout. The following grid sizes were used:
\begin{verbatim}
  Config.pl -g=GM:8,8,8,200,40 -g=SC:4,4,4,1000 
  Config.pl -g=UA:9,9,25,2,2   -g=SP:1000,10,150
\end{verbatim}
Note that the GM, SC and UA grid sizes differ from the defaults.
If the IH component is BATSRUS\_share, it uses the same source
code as GM/BATSRUS, and hence the same grid. If IH/BATSRUS is
selected, then one can use different grids, for example
\begin{verbatim}
  Config.pl -g=GM:8,8,8,100,40 -g=IH:8,8,8,200,1
\end{verbatim}
since GM needs only a 100 blocks per PE (if run on at least
25 processors), while IH does not need any implicit blocks 
(see the BATSRUS manual for details).
Note, however, that this will save memory only if the large arrays used
by a given component are allocated only on the processors which are dedicated 
for that component. The BATSRUS code uses static allocation by
default, but it can also use dynamic allocation. To make for example
the GM and SC components dynamic type
\begin{verbatim}
  cd GM/BATSRUS/src; make DYNAMIC
  cd ../../..
  cd SC/BATSRUS/src; make DYNAMIC
  cd ../../..
\end{verbatim}
Now compile the source code into bin/SWMF.exe 
and create the post processing executable bin/PostIDL.exe 
and the run directory with the following commands
\begin{verbatim}
  make
  make PIDL
  make rundir
\end{verbatim}

\subsubsection{Parameter, Layout and Restart Files}

Open the prepacked SC+IH and the GM+IM+IE+UA restart files in
the run directory.
On a ``little endian'' platform (Linux clusters, SGI Altix and Compaq)
\begin{verbatim}
  cd run
  tar xzf ../test/scih_4hr_restart.tgz
  tar xzf ../test/gmimieua_restart.tgz
\end{verbatim}
while on a ``big endian platform'' (SGI Origin, MAC) use
\begin{verbatim}
  cd run
  gunzip ../test/scih_4hr_restart_sgi.tgz
  gunzip ../test/gmimieua_restart_sgi.tgz
  tar xf ../test/scih_4hr_restart_sgi.tar
  tar xf ../test/gmimieua_restart_sgi.tar
\end{verbatim}
Note that the {\tt tar} program on the SGI does not recognize the 
{\tt -z flag}, so we have to decompress the files with {\tt gunzip} 
first.

Copy the PARAM and LAYOUT files into the run directory
\begin{verbatim}
  cp ../test/PARAM.in.8comp PARAM.in
  cp ../test/LAYOUT.in.8comp LAYOUT.in
\end{verbatim}
Edit the LAYOUT.in file according to the number of processors
available. The file in its original form is best suited for 
running on around 128 processors or more.
Note that for the above suggested grid sizes and the
selected component versions the following restrictions apply:
\begin{itemize}
\item The IM/RCM component can use 1 processor only
\item The RB/RiceV5 component can use 1 processor only
\item The SP/Kota component can use 1 processor only
\item The IE/Ridley\_serial component can use 1 or 2 processors.
\item The UA/GITM component can use at most 32 processors, 
      and needs at least 8 processors. The number of blocks should
      be the same on all processors, so use 8, 16 or 32 processors.
\item The GM/BATSRUS component needs at least 25 processors.
\item The SC/BATSRUS component needs at least 23 processors.
\item The IH/BATSRUS(\_share) component needs at least 25 processors.
\item The IH/BATSRUS\_share component cannot be overlapped with the
      GM/BATSRUS component.
\end{itemize}
Once the LAYOUT.in file has been edited, make sure that everything
is correctly setup by running the TestParam.pl script. 
If you plan to run the test on 128 processors, type
\begin{verbatim}
  Scripts/TestParam.pl -n=128
\end{verbatim}
in the main SWMF directory.
If any errors or warnings were reported, fix them until the script
runs silently. 

\subsubsection{Running the Test}

Now you are ready to run the test interactively as
\begin{verbatim}
cd run
mpirun -np 128 SWMF.exe
\end{verbatim}
or as a job. The test involves 2 sessions, each runs for 300 seconds
real time. 

On 128 processors of an SGI Altix machine (altix2 at NASA Ames) 
the 2 sessions finished in 278 and 240 seconds, 
which is 8\% and 25\% faster than real time, respectively.
Note that the timings do not include the set up time, which was 217 seconds.
For a longer run we expect that the performance observed in the
second session would persist.

\subsection{The 5 Component Test}

This test was developed for Milestone 7I. 
It can be done in the current version as well with minor modifications. 
The tests are in the SMWF\_7I\_TEST CVS repository.  
The following tests are available:
\begin{itemize}
\item {\tt StartTst.tgz}\\
      Steady state run for IH, GM and IE components.
      The run directory is created with the same files as in
      this test.

\item {\tt RestartTst.tgz}\\
      Time accurate run for IH-GM-IM-IE-UA component
      The restart files are for little endian platforms.

\item {RestartTstSGI.tgz}\\
      Same as {\tt RestartTst.tgz} 
      but for big endian platforms such as the SGI.
\end{itemize}
On 16 CPU-s of a Linux cluster (AMD Athlon 1900+, using a 100Mbs
network) the StartTst required 2872 seconds to complete, while the
restart test requires 182 seconds (set up time is not included).

Reference results are also  provided:
\begin{itemize}
\item {\tt StartResult.tgz}\\
       Results for the steady state run with IH, GM and IE

\item {\tt RestartResultGeneral.tgz}\\
      Results for the restarted run using the 
      LAYOUT.in.general and PARAM.in.general files.
\end{itemize}
Only the ASCII output files are included for sake of easy comparison.
Results may vary slightly from platform to platform due to round off errors.


\subsubsection{Running the startup test}

To run this test you need to install, configure and compile SWMF.
The {\tt IH/BATSRUS} (or {\tt BATSRUS\_share}), 
{\tt GM/BATSRUS} and {\tt IE/Ridley\_serial}
components are required, but it is useful to include all five
components so the same executable and run directory 
can be used for the next test too. For example set
\begin{verbatim}
  Config.pl -v=Empty
  Config.pl -v=IH/BATSRUS_share,GM/BATSRUS,IM/RCM,IE/Ridley_serial,UA/GITM
  Config.pl -g=GM:8,8,8,400,1 -double
\end{verbatim}
The default parameter and layout files can be used. 
These are copied into the run directory during {\tt make rundir},
or can be copied with
\begin{verbatim}
  cd run
  cp Param/PARAM.DEFAULT  PARAM.in
  cp Param/LAYOUT.DEFAULT LAYOUT.in
\end{verbatim}
This test takes some time, and it creates the restart files
for the following test. 

After the run finished, you may post process the IDL plot files (the
{\tt .idl} and {\tt .h} files produced by GM and IH can be processed
into {\tt .out} files), and the northern and southern hemispheres
plots of the IE component can be put together: 
\begin{verbatim}
  cd run/GM
  ./pIDL
  cd ../IH
  ./pIDL
  cd ../IE
  ./pION
\end{verbatim}
The {\tt .out} files can be visualized with IDL using the scripts in
the Idl directory.  You can also compare the ASCII output files with
the ones given in the reference solutions.

\subsubsection{Running the short restart test}

To run this test you need to install, configure and compile SWMF
as described above and create the run directory. 
The {\tt IH/BATSRUS} (or {\tt BATSRUS\_share}),
{\tt GM/BATSRUS}, {\tt IM/RCM}, {\tt IE/Ridley\_serial} and
{\tt UA/GITM} components are required. The configuration shown
in the previous section can be used, for example.
After this you can install the test into the run directory.  
For example on a Linux or TrueUnix
(OSF1) machine the short restart test can be installed with the
following commands:
\begin{verbatim}
  cd run
  tar xzf ../Tests/RestartTst.tgz
  cp Param/PARAM.in.test.restart.IHGMIMIEUA PARAM.in
  cp Param/LAYOUT.in.test.restart.IHGMIMIEUA LAYOUT.in
\end{verbatim}
On a big endian platform use the {\tt RestartTstSGI.tgz} file instead.

Now you can run the code interactively or submit a job.  The default
layout assumes 16 PE-s. For example an interactive execution could be
like this:
\begin{verbatim}
  mpirun -np 16 SWMF.exe
\end{verbatim}
The post processing of the plot files is the same as described above. 
The IM/RCM and UA/GITM components do not require post processing.

\subsubsection{Test with the renamed IH code}

The restriction on the overlapping of the IH and GM components
can be eliminated by the use of the renamed IH source code. 
To test this, select the {\tt IH/BATSRUS} component:
\begin{verbatim}
  Config.pl -v=IH/BATSRUS
\end{verbatim}
This will automatically create the renamed source code for IH/BATSRUS.
which will take 2-5 minutes. Once the renaming
is done, reduce the number of the blocks for GM and IH:
\begin{verbatim}
  Config.pl -g=GM:8,8,8,200,1 -g=IH:8,8,8,200,1
\end{verbatim}
The test requires 520 blocks for GM and 456 blocks for IH on the PE-s
assigned to the components. So with at least 3 PE-s for GM and IH each,
the above numbers are sufficient. This test does not require the implicit
scheme, therefore the number of blocks with implicit time stepping can
be minimized. You can specify different number of blocks for GM and IH. 

Recompile the code:
\begin{verbatim}
  make
\end{verbatim}
Now it is allowed to overlap GM and IH in the {\tt LAYOUT.in} file.
The renamed IH code has the version name {\tt IH\_BATSRUS}, 
which is printed to the standard output at the
beginning of the execution.
The results should remain identical with those obtained with the 
{\tt IH/BATSRUS\_share} component version.

\section{Test-Requirements Matrix}

The following table shows the correspondance between the requirements
(listed in the REQUIREMENTS document) and the tests in this document.
Only the requirements that can be tested with a test run are listed.
The first column of the table shows the identifier of the requirement,
the second column describes the test with words, and the last column
gives the section number in this document.

\vspace{1cm}
\begin{center}
\input test_matrix
\end{center}

\end{document}
