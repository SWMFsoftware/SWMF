%\documentclass[a4paper,11pt]{article}
%\author{\bf Center for Space Environment Modeling, The University of Michigan}
%\title{\bf \Large Release Notes for the Milestone 7I and Reference Manual}
%\maketitle

\chapter{Introduction}

This document describes a working prototype of the 
Space Weather Modeling Framework (SWMF).
The SWMF was developed to provide a flexible tool serving the Sun-Earth
modeling community.  In its current form the SWMF contains many 
domains extending from the surface of the Sun to the upper atmosphere of the 
Earth: 
\begin{enumerate}
\item CZ -- Convection Zone
\item EE -- Eruptive Event generator
\item GM -- Global Magnetosphere 
\item IE -- Ionosphere Electrodynamics
\item IH -- Inner Heliosphere
\item IM -- Inner Magnetosphere
\item OH -- Outer Heliosphere
\item PC -- Particle-in-Cell
\item PS -- Plasma Sphere (under development)
\item PT -- Particle Tracker
\item PW -- Polar Wind
\item RB -- Radiation Belts
\item SC -- Solar Corona
\item SP -- Solar Energetic Particles 
\item UA -- Upper Atmosphere
\end{enumerate}
The core of the SWMF and most of the models are implemented in Fortran90.
Some models are written in Fortran 77, and others are written in C++.
A few features of Fortran 2003 and 2008 are also used.
The parallel communications use the Message Passing Interface (MPI) library. 
Some models can also take advantage of multi-threaded execution using the 
OpenMP library. A significant fraction of the SWMF has been ported to GPUs
using the OpenACC library.
The SWMF creates a single executable. Note, however, that the
models in the SWMF can still be compiled into stand-alone executables.
This means that the models preserve their individuality while being
compatible with the SWMF.

\section{Acknowledgments}

The first version of the SWMF was developed at the Center for 
Space Environment Modeling (CSEM) of the University of Michigan under 
the NASA Earth Science Technology Office (ESTO) 
Computational Technologies (CT) Project (NASA
CAN NCC5-614). The project was entitled as ``A High-Performance
Adaptive Simulation Framework for Space-Weather Modeling (SWMF)''.
The Project Director was Professor Tamas Gombosi, and the Co-Principal
Investigators are Professors Quentin Stout and Kenneth Powell.

The first version of the SWMF and many of the physics components 
were developed at CSEM by the following individuals (in alphabetical order):
David Chesney, Yue Deng,
Darren DeZeeuw, Tamas Gombosi, Kenneth Hansen, Kevin Kane, Ward (Chip)
Manchester, Robert Oehmke, Kenneth Powell, Aaron Ridley, Ilia Roussev,
Quentin Stout, Igor Sokolov, G\'abor T\'oth and Ovsei Volberg.

The core design and code development was done by G\'abor
T\'oth, Igor Sokolov and Ovsei Volberg:
\begin{itemize}
\item Component registration and layout was designed and implemented by 
      Ovsei Volberg and G\'abor T\'oth.
\item The session and time management support as well as the various
      configuration scripts and the parameter editor GUI were designed and
      implemented by G\'abor T\'oth.
\item The SWMF coupling toolkit was developed by Igor Sokolov.
\item The SWMF GUI was designed and implemented by Darren De Zeeuw.
\end{itemize}
The SWMF has undergone major improvements over the years. 
The current version has a fully automated test suite designed
by G\'abor T\'oth. 
Empirical models were added in a systematic way.
The shared libraries and utilities have been greatly extended.
The SWMF has been coupled to the Earth System Modeling Framework (ESMF).

The current physics models were developed by the following research groups
and individuals:
\begin{itemize}
\item
The Eruptive Event generator (EE), Solar Corona (SC), Inner Heliosphere (IH), 
Outer Heliosphere (OH), and the Global Magnetosphere (GM) 
components are based on \BATSRUS\ MHD code developed at CSEM. 
\BATSRUS\ is a highly parallel up to 3-dimensional block-adaptive 
hydrodynamic and MHD code. 
Currently the main developers of \BATSRUS\ are Gabor Toth,
Bart van der Holst, and Igor Sokolov.
The current version of the Solar Corona model was developed
by Bart van der Holst, Chip Manchester, Igor Sokolov with contributions
from Cooper Downs, Ilia Roussev and Ofer Cohen. 
The Inner Heliosphere model was mostly developed by Chip Manchester.
The Outer Heliosphere model was developed by Merav Opher and Gabor Toth. 
The Global Magnetosphere model was developed by 
Darren De Zeeuw, Gabor Toth, Aaron Ridley and many others.
The physics based Eruptive Even Generator model is also based on
\BATSRUS. It is developed by Fang Fang, Chip Manchester and Bart van
der Holst. The EE model already works as a stand-alone code, and
it will by coupled to other components in the SWMF in the future.

\item
The Ionospheric Electrodynamics (IE) model is the Ridley Ionosphere Model
developed by Aaron Ridley, Darren De Zeeuw and Gabor Toth at CSEM.
RIM  is a 2-dimensional spherical electric potential solver.
It has two versions. The Ridley\_serial version can run on up to 2 processors,
the RIM version is fully parallel with many new options, however it is still
being developed, and it is not yet fully functional.

\item
The first Inner Magnetosphere (IM) model in the SWMF was
the Rice Convection Model (RCM) developed Dick Wolf, Stan Sazykin and 
others at Rice University, also modified by Darren De Zeeuw at the
University of Michigan. The current version of the model with oxygen and loss
is named RCM2. The RCM code is 2-dimensional in space 
(plus one dimension for energy) and serial. There are 3 more IM models.
The Comprehensive Inner Magnetosphere and Ionosphere (CIMI) model 
was developed by Mei-Ching Fok, Natasha Buzulukova and 
Alex Glocer at the NASA Goddard Space Flight Center.
In addition to inner magnetosphere, CIMI also includes the high
energy electrons of the radiation belt
The HEIDI model was developed by Mike Liemohn and Raluca Ilie 
at the University of Michigan.
The RAM-SCB model was developed by Vania Jordanova, Sorin Zaharia and
Dan Welling at Los Alamos.
The CIMI, HEIDI and RAM-SCB models are also 2 dimensional
in space, but they resolve energy as well as pitch angle.

\item
There are three Particle-in-Cell (PC) models in the SWMF. 
All of them use an energy conserving semi-implicit PIC algorithm.
The iPIC3D code was originally developed by Stefano Markidis and
his group at KTH Sweden
and Giovanni Lapenta and his group at KU Leuven, Belgium.
The code was adapted, integrated into the SWMF and coupled
with BATS-R-US by Lars Daldorff and Gabor Toth. 
iPIC3D solves for the electric and magnetic fields on 
a 3D Cartesian grid and the motion of electron and ion
macroparticles. iPIC3D is a parallel code written in C++. 
Further improvements were made by Yuxi Chen and Gabor Toth 
to make it energy and charge conserving.
The Adaptive Mesh Particle Solver (AMPS) code, was developed by 
Valeriy Tenishev. It was modified into a PIC solver by Valeriy Tenishev
and Yinsi Shou with help from Yuxi Chen. The Flexible Exascale Kinetic
Solver (FLEKS) was developed by Yuxi Chen. Both AMPS and FLEKS allow
the PIC region to be adapted during the run. Currently FLEKS is the main
PIC model.

\item
The Polar Wind (PW) component is the Polar Wind Outflow Model (PWOM)
developed by Alex Glocer, Gabor Toth and Tamas Gombosi
at the University of Michigan.  This code solves the
multifluid equations along multiple field lines and it is fully parallel.

\item
The Particle Tracker (PT) model is the Adaptive Mesh Particle Scheme (AMPS).
The main developer of AMPS is Valeriy Tenishev. AMPS solves the motion
and interaction of neutral and charged particles on an up to 3D 
adaptive grid. AMPS is a parallel code written in C++.
FLEKS can also be used as a PT model.

\item
The Radiation Belt Environment (RBE) model is developed by Meiching Fok
and Alex Glocer at NASA Goddard. It is a spatially 2-dimensional code with 
extra two dimensions for pitch angle and energy. It is essentially the
same as the energetic electron solver in the CIMI inner magnetosphere model.

\item
One of the Solar Energetic Particle (SP) models is the
K\'ota's SEP model by Joseph Kota at the University of Arizona.
It solves the equations for the advection and acceleration of
energetic particles along a magnetic field line in a 3D phase space
using energy and pitch angle as the extra two dimensions.
The other SP model is the Multiple Field Line Advection model for 
Particle Acceleration (MFLAMPA) by Igor Sokolov and Dmitry Borovikov. 
MFLAMPA also solves
for energy distribution but assumes an isotropic pitch angle distribution.

\item
The Upper Atmosphere (UA) model is the 
Global Ionosphere-Thermosphere Model (GITM) 
developed by Aaron Ridley, Yue Deng, and Gabor Toth at CSEM.
GITM is 3-dimensional spherical
fully parallel hydrodynamic model with ions, neutrals, chemistry etc.
The Mars version of GITM (MGITM) is being developed by Dave Pawlowski.

\end{itemize}
The following empirical models are available:
\begin{itemize}
\item[EEE] The Gibbson-Low and the Titov-Demoulin flux rope models
          can be used to initiate CME-s. The breakout model is
          also available. The STITCH model is a recent addition.
\item[EGM] The Tsyganenko 1996 and 2004 models.
\item[EIE] The Weimer 1996 and 2000 models, and many more empirical
          ionospheric electrodynamics models.
\item[EUA] The MSIS and IRI models for the upper atmospher and the 
          ionosphere, respectively.
\end{itemize}

\section{The SWMF in a Few Paragraphs}

The SWMF is a software framework that allows integration
of various models into a coherent system. The SWMF allows
running and coupling any meaningful subset of the models together.
The main applications of the SWMF are related to space
physics and space weather, but it can be, and has been, used for 
other applications that the models allow.
The SWMF contains a Control Module (CON), which is responsible for
component registration, processor layout for each component as well
as initialization, execution and coupling schedules.  
The SWMF contains templates (examples) and utilities for 
integrating and coupling models. 
A component is adapted from user-supplied physics codes,
(for example \BATSRUS\ or RCM), by adding two relatively small units
of code:
\begin{itemize}
\item A wrapper, which provides the control functions for CON, and
\item A coupling interface to perform the data exchange with other
components.
\end{itemize}
Both the wrapper and coupling interface are constructed from the
building blocks provided by the framework. 

An SWMF component is compiled into a separate library that resides in
the directory {\tt lib}, which is created as part of the installation
process described later in this document. The executable {\tt SWMF.exe}
is created in the
directory {\tt bin}, which is created during the compilation.  If a
user does not want to use some particular component, this component
should be configures to an empty version of the component.

The framework controls the initialization, execution, coupling and
finalization of components.  The execution is done in sessions. In
each session the parameters of the framework and the components can be
changed. The list of usable (non-empty) components and their 
processor layout (the array of processors used by that component)
and all other input parameters are read from the {\tt PARAM.in} file, which
may contain further included parameter files.  These parameters are
read and broadcast by CON and the component specific parameters are
sent to the components. The structure of the parameter file will be
described in detail.

If two components reside on different sets of processing elements
(PE-s) they can execute in an efficient concurrent manner.
This is possible, because the coupling times (in terms of the simulation time
or number of iterations) are known in advance.  
The components advance to the time of coupling and
only the processors involved in the coupling need to communicate with
each other. The components are also allowed to share some processing elements.
The execution is sequential for the components with overlapping layouts.
This can be useful when the execution time of the components vary a lot
during the run, or when a component needs a lot of processors 
for memory storage, but it requires little CPU time.
Of course, this still allows the individual components to execute in parallel.
For steady state calculations the components are allowed to progress
at different rates towards steady state. Each component can be called
at different frequencies by the control module.

The coupling of the components is done through the SWMF. 
Couplings are done individually betwee one source and one target component.
The frequency of couplings is determined by the input parameters.
Two-way coupling is performed as two separate couplings
with the source and target roles reversed.
Each coupling only involves the processors used by either the
source or target component. All other processors can keep working
on their tasks.
There are several utilities that facilitate component couplings. 
The most basic approach relies on plain MPI calls using the various 
functions of the SWMF that define the MPI communicators
and information about the layout. While this approach is general,
writing correct and efficient MPI code is not easy. The following
libraries can make coupling much easier.

Passing scalars and small arrays between
the source and target components can be done easily with the functions defined in 
CON\_transfer\_data. Here the coupling consists of
\begin{itemize}
\item an optional data reduction (MPI\_SUM) on the source component
\item single source processor to single target processor data transfer
\item an optional data broadcast on the processors of the target component
\end{itemize}
For large amounts of data distributed over many processors, the 
CON\_couple\_points utility can be used with efficient N to M parallel
communication. The point coupling algorithm consists of the following steps
\begin{itemize}
\item checking if the communication pattern needs to be updated
\item if yes, then target model sends the list of point coordinates 
      to the source model and the source model responds with the processor
      indexes.
\item source processors interpolates and send their data to the appropriate
      target processors directly
\end{itemize}
Finally, the SWMF coupling toolkit also allows N to M coupling between 
components that describe their grids for the SWMF.
Temporal interpolation is not directly supported by any of these
utilities, but the components can do that internally.

\section{System Requirements}

In order to install and run the SMWF the following minimum system
requirements apply.
\begin{itemize}
\item The SWMF runs only under the UNIX/Linux operating systems.  This now
  includes Macintosh system 10.x because it is based on BSD UNIX.  The
  SWMF does not run under any Microsoft Windows operating system.
\item A FORTRAN 2008 compiler must be installed.
\item A C/C++ compiler must be installed.
\item The Perl interpreter must be installed.
\item A version of the Message Passing Interface (MPI) library must be
  installed for parallel execution.
\item Some models can use the OpenMP library. The Fortran and C++ compilers
  need to contain OpenMP to use this feature.
\item Some models use HDF5 output. For these the parallel version of 
  the HDF5 library has to be installed. The {\tt share/Scripts/install\_hdf5.sh}
  shell script can be used to do that.
\item Some models use the SPICE library to determine position and 
  orientation of various objects. To use these features, the SPICE
  library needs to be installed.
\item In order to generate the documentation, LaTex has to be installed.
  The PDF generation requires the {\tt dvips} and {\tt ps2pdf}
  utilities. 
\end{itemize}
One may be able to compile the code and do very small test
runs on 1 or 2 processor machines.  However, to do most physically
meaningful runs the SWMF requires a parallel processor machine with a 
minimum of 8 processors and a minimum of 8GB of memory.
Very large runs require many more processors.

The framework has been ported to many platforms using many different
compilers and MPI libraries. The list of Fortran compilers includes
Absoft, GNU gfortran, Intel ifort, Lahey, NAG nagfor, PGI pgf90, NVIDIA nvfortran, and IBM xlf90.
The list of C compilers include GNU gcc, Apple clang, Intel icc, PGI pgcc, IBM mpxlc.
The MPI libraries used so far include many versions of MPICH, MVAPICH and 
OpenMPI. 

In addition to the above requirements, the SWMF output is typically
visualized using IDL, Tecplot, python (SpacePy), VisIt, Paraview or yt.
Other visualization packages may also be used, but the output file
formats and scripts have been designed for these visualization softwares.

%-----------------------------------------------------------------------
% Chapter 2
%-----------------------------------------------------------------------

\chapter{Quick Start}

\section{A Brief Description of the SWMF Distribution}

The distribution in the form of the compressed tar image
includes the SWMF source code.
The top level directory contains the following subdirectories:
\begin{itemize}\itemsep=0pt
\item {\tt CON}     - the source code for the control module of the SWMF
\item {\tt Copyrights} - copyright files
\item {\tt ESMF}    - the ESMF wrapper for the SWMF
\item {\tt CZ, EE, ... UA} - component directories
\item {\tt Param}   - example input parameter files
\item {\tt Scripts} - shell and Perl scripts
\item {\tt doc}     - the documentation directory %^CMP IF DOC
\item {\tt output}  - reference test results for the SWMF tests
\item {\tt share}   - shared scripts and source code
\item {\tt util}    - utilities such as TIMING, NOMPI, empirical models, etc.
\end{itemize}
and the following files
\begin{itemize}\itemsep=0pt
\item {\tt README.md}     - a short instruction on installation and usage
\item {\tt PARAM.XML}     - description of CON parameters
\item {\tt Makefile}      - the main makefile
\item {\tt Makefile.test} - the makefile containing the tests %#^CMP IF TESTING
\item {\tt Config.pl}     - Perl script for (un)installation and configuration
\end{itemize}

\section{General Hints}

\subsubsection{Getting help with scripts and the Makefile}

Most of the Perl and shell scripts that are distributed with the SWMF
provide help which can be accessed wit the {\tt -h} flag. For example, 
\begin{verbatim}
  ./Config.pl -h
\end{verbatim}
will provide a detailed listing of the options and capabilities of the
{\tt Config.pl} script.  In addition, you can find all the possible
targets  that can be built by typing
\begin{verbatim}
make help
\end{verbatim}

\subsubsection{Input commands: PARAM.XML}

The input commands used in the {\tt PARAM.in} and their meaning are 
described in the {\tt PARAM.XML} files. For the SWMF itself it is found in
\begin{verbatim}
PARAM.XML
\end{verbatim}
while the files for the physics components are found in the component's
subdirectory.  For example, the file for the GM/BATSRUS component can
be found at
\begin{verbatim}
GM/BATSRUS/PARAM.XML
\end{verbatim}
This file contains a complete list of all input commands for the
component as well as the type, the allowed ranges and default values
for each of the input parameters.
Although the XML format makes the files a little hard to read, they are
extremely useful.  A typical usage is to cut and paste commands out of the
PARAM.XML file into the PARAM.in file for a run. 

An alternative approach is to use the web browser based parameter editor 
to edit the PARAM.in file for the SWMF 
(also for the stand-alone models that have PARAM.XML files).
The editor GUI can be started as
\begin{verbatim}
share/Scripts/ParamEditor.pl
\end{verbatim}
This editor allows constructing {\tt PARAM.in} files with pull down menus, 
shows the manual for the edited commands, and checks the correctness of
the parameter file and highlights the errors. All this functionality 
is based on the PARAM.XML files.

\subsubsection{Have the working directory in your path}

In order to run executable files in the UNIX environment, either
the current working directory has to be in the path or the filename has
to be typed with the path.
In UNIX the current working directory is represented
by the period (.).  For example
\begin{verbatim} 
./Config.pl -s
\end{verbatim}
will execute the Config.pl script if it is in your current directory.  
If the `.' is added to the path, for example with
\begin{verbatim}
set path = (${path} .)
\end{verbatim}
then one can simply type
\begin{verbatim} 
Config.pl -s
\end{verbatim}
Setting the path is best done in the .tcshrc or equivalent Unix shell 
customization file located in the user's home directory.

\section{Installation}

The installation instructions are described in the README.md file.
To keep this user manual more up-to-date and consistent, 
the README.md file is quoted verbatim below.

\verbinput{../../README.md}

\section{Platform specific information}

The SWMF is tested every night on several computers.
The test page can be found at
\begin{verbatim}
  http://csem.engin.umich.edu/SWMFTESTS
\end{verbatim}
This page shows which tests passed and which tests failed.
It also shows the combination of compilers and libraries
used on various machines (click on the Explanations link at the top).

If you are running on one of the tested machines, 
you can use the {\tt module load XYZ} command to load 
the appropriate modules. It is best to do this in the {\tt .tcshrc}
or equivalent customization file in the home directory. Use
\begin{verbatim}
   module avail
\end{verbatim}
to see the list of all available modules.

\section{Building and Running an Executable}

At compile time, the user can select the physics models to be
compiled.  All components with an {\tt Empty} model version will be 
unavailable for use at run time.  
The physics components can be selected with the {\tt -v} flag
of the Config.pl script. For example typing
\begin{verbatim}
  Config.pl -v=Empty,SC/BATSRUS,IH/BATSRUS,SP/Kota
\end{verbatim}
will select BATSRUS for the SC and IH components and K\'ota's model for
the SP componens. All the other components are set to 
the {\tt Empty} model versions that contain empty subroutines for compilation, 
but cannot be used. Note that {\tt Empty} always has to be listed first.
The default configuration uses the Empty version for all components.

The grid size of several components can also be set with the {\tt -g}
flag of the {\tt Config.pl} script. For example the 
\begin{verbatim}
  Config.pl -g=GM:8,8,8
\end{verbatim}
command sets the block size for the GM component to $8\times 8\times 8$ cells.
The main SWMF Config.pl script actually runs the individual Config.pl
scripts in the component versions. These scripts can be run directly,
For example try
\begin{verbatim}
  cd GM/BATSRUS
  Config.pl -show
\end{verbatim}
Compilation rules, library definitions, debugging flags, and optimization 
level are stored in {\tt Makefile.conf}. This file is created during
installation of the SWMF and contains default settings for production runs.
The compiler flags can be modified with
\begin{verbatim}
  Config.pl -debug -O0
\end{verbatim}
to debug the code with 0 optimization level, and
\begin{verbatim}
  Config.pl -nodebug -O4
\end{verbatim}
to run the code at maximum optimization level and without the debugging flags.
Before compiling the SWMF, it is always a good idea to check its configuration
with
\begin{verbatim}
  Config.pl -show
\end{verbatim}
To build the executable {\bf bin/SWMF.exe}, type:
\begin{verbatim}
  make -j
\end{verbatim} 
The -j flag allows parallel compilation, which can reduce the compilation
time considerably.
Depending on the configuration, the compiler settings and the machine, 
compiling the code can take several minutes. 
In addition, the PostIDL.exe post processing code can be compiled with
\begin{verbatim}
  make PIDL
\end{verbatim} 

The {\tt SWMF.exe} executable should be run in a sub-directory, 
since a large number of files are created in each run.  
To create this directory use the command:
\begin{verbatim}
  make rundir
\end{verbatim} 
This command creates a directory called {\tt run}.  You can either
leave this directory as named, or {\tt mv} it to a different name.  It
is best to leave it in the same SWMF directory, since
keeping track of the code version associated with each run is quite
important. On some platforms, however, the runs should be done on a
parallel file system (often called scratch or nobackup), while the
source code is better kept in the home directory. In this case move
the run directory to the scratch disk and create a symbolic link to it, 
for example
\begin{verbatim}
  mv rundir /p/scratch/MYNAME/SWMF/run_halloween2
  ln -s /p/scratch/MYNAME/SWMF/run_halloween2 .
\end{verbatim}
The {\tt run} directory will contain links to the codes
which were created in the previous step as well as subdirectories
where input and output of the different components will reside.
On some systems the compute nodes cannot access symbolic links
across different file systems. In this case the executable should be 
copied instead of linked, so in our example the following commands
should be done every time after the SWMF has been (re)compiled:
\begin{verbatim}
  rm -f run_halloween2/SWMF.exe
  cp bin/SWMF.exe run_halloween2/
\end{verbatim}
To run the SWMF change directory into the {\tt run} directory 
(or the symbolic link to it):
\begin{verbatim}
  cd run_halloween2
\end{verbatim}
In order to run the SWMF you must have the input file PARAM.in.
The required \#COMPONENTMAP command in the PARAM.in file defines the processor
layout for the components involved in the future run.  In addition,
the PARAM.in file contains the detailed commands for controlling what you want the
code to do during the run.  The {\tt Param} directory contains many
example input files. Many of these are used by the nightly test suite.

An example processor map to run the executable with
five components on 16 processors is:
\begin{verbatim}
ID  First Last Stride
#COMPONENTMAP
GM  0  4  1       CompMap
IE  5  6  1       CompMap
IH  7 10  1       CompMap
IM 11 11  1       CompMap
UA 12 15  1       CompMap
\end{verbatim}
The syntax is simple. It must start with the directive
\#COMPONENTMAP followed by the processor layout of active components.
Each line specifies the label for component, i.e. IE, GM and
etc., its first and last processors, all relative to the world
communicator, and the stride. Thus GM will run on 5 processors from 0
to 4, and IM will run on only 1 processor, the processor 11.  If
stride is not equal to 1, the processors for the component will not be
neighboring processors.

It is strongly recommended to check the validity of the {\tt PARAM.in} 
file before running the code. If the code will be run on 16 processors, type
\begin{verbatim}
Scripts/TestParam.pl -n=16 run_halloween2/PARAM.in
\end{verbatim}
in the main SWMF directory.
The Perl script reports inconsistencies and errors. 
If no errors are found, the script finishes silently.
Now you are ready to run the executable through submitting a batch job or, 
if it is possible on your computer, you can run the code interactively.  For
example, to run the SWMF interactively:
\begin{verbatim}
cd run_halloween2
mpiexec -n 16 SWMF.exe | tee runlog
\end{verbatim}
The {\tt | tee runlog} splits the output and send it to the screen
as well as into the {\tt runlog} file.
The SWMF provides example job scripts for several machines. 
These job script files are found in the
\begin{verbatim}
share/JobScripts/
\end{verbatim}
directory. If the name of some of the job script files matches the
name of the machine returned by the {\tt hostname} command (numbers
at the end of the machine name are ignored, so pfe23 matches job.pfe), 
the job script is copied into the {\tt run} directory when it is created.
These job scripts serve as a starting point only, they must
be customized before they can be used for submitting a job.

To recompile the executable with different compiler settings you have
to use the command
\begin{verbatim}
make clean
\end{verbatim}
before recompiling the executables. It is possible to recompile
only a component or just one subdirectory if the {\tt make clean}
command is issued in the appropriate directory.

\section{Post-Processing the Output Files}

Several components produce output files (plot files) that require
some post-processing before they can be visualized. The post-processing
collects data written out by different processors, and it can also
process and transform the data. 

The PostProc.pl script greatly simplifies the post-processing and
it also helps to collect the run results in a well-structured directory tree.
The script can also be used to do post-processing while the code is running.
Usually the processed output files are much smaller than the raw output file,
so post-processing during the run can limit the amount of disk space used
by the raw data files. It also avoids the need to wait for a long time 
for the post-processing after the run is done. 

The {\tt PostProc.pl} script is copied into the run directory and it should
be executed from the run directory.
To demonstrate the use of the script, here are a few simple examples.
After or during a run, you may simply type
\begin{verbatim}
cd run_halloween2
./PostProc.pl
\end{verbatim}
to post-process the available output files. The series of individual 
IDL plot files can be concatenated into single movie files with
\begin{verbatim}
./PostProc.pl -M
\end{verbatim}
Repeat the post-processing every 360 seconds during the run,
and gzip large ASCII files:
\begin{verbatim}
./PostProc.pl -r=360 -g >& PostProc.log &
\end{verbatim}
After the run is finished, create IDL movie files and concatenate
various log and satellite files (for restarted runs),
and create a directory tree {\tt RESULTS/NewRun} with the output
of all the components, the input parameter file, 
a restart directory tree (if restart information was saved), 
and the {\tt runlog} file (if present):
\begin{verbatim}
./PostProc.pl -M -cat RESULTS/NewRun
\end{verbatim}
The {\tt RESULTS/NewRun} directory will contain the PARAM.in file, the
runlog file (the standard output should be piped into that file),
the restart directory named {\tt RESULTS/NewRun/RESTART/},
and the output files for each component in a subdirectory named
accordingly (eg. {\tt RESULTS/NewRun/GM/}). The output directories of
the components (e.g. {\tt GM/plots/}) will be empty after this.

To see all the options of the script, including parallel processing and
syncing results to a remote computer, type
\begin{verbatim}
./PostProc.pl -h
\end{verbatim}

\section{Restarting a Run}

There are several reasons for restarting a run. A run may fail
due to a run time error, due to hardware failure, due to 
software failure (e.g. the machine crashes) or because the
queue limits are exceeded. In such a case the run can be continued from
the last saved state of SWMF. 

It is also possible that one builds up a complex simulation from multiple 
runs. For example the first run creates a steady state for the SC component.
The second run includes both the SC and IH components and it 
restarts from the results of the first run and creates a steady state
for both components. A third run may restart from this solution and include
the GM component, etc. 

The restart files are saved at the frequency determined in the PARAM.in file.
Normally the restart files are saved into the output restart directories
of the individual components and subsequent saves overwrite the previous ones
(to reduce the required disk space). A restart requires the modification
of the PARAM.in file: one needs to include the restart file for the
control module of SWMF as well as ask for restart by all the components.

The Restart.pl script simplifies the work of the restart in several ways:
\begin{enumerate}
\item The SWMF restart file and the individual output restart 
directories of the components are collected into a single directory tree, 
the {\bf restart tree}.
\item The default input restart file of SWMF and the default 
      input directories of the components can be linked to an existing
      restart tree.
\item The script can run continuously in the background and create
      multiple restart trees while SWMF is running. 
\item The script does extensive checking of the consistency 
      of the restart files.
\end{enumerate}
The Restart.pl script is copied into the run directory and it should
be executed in the run directory. Note that the PARAM.in file is not
modified by the script: it has to be modified with an editor as needed.

To demonstrate the use of the script, here are a few simple examples.
After a successful or failed run which should be continued, simply type
\begin{verbatim}
cd run_halloween2
./Restart.pl
\end{verbatim}
to create a restart tree from the final output and to link to the tree for the
next run. The default name of the restart tree is based on the simulation time
for time accurate runs, or the time step for non-time accurate runs.
But you can also specify a name explicitly, for example
\begin{verbatim}
./Restart.pl RESTART_SC_steady_state
\end{verbatim}
If you wish to continue the run in another run directory, or on another
machine, transfer the restart tree as a whole into the new run
directory and type
\begin{verbatim}
./Restart.pl -i=RESTART_SC_steady_state
\end{verbatim}
where the {\tt -i} stands for ``input only'', i.e. the script links to
the tree, but it does not attempt to create the restart tree.

To save multiple restart trees repeatedly at an hourly frequency of 
wall clock time while the SWMF is running, type
\begin{verbatim}
./Restart.pl -r=3600 &
\end{verbatim}
To see all the options of the script type
\begin{verbatim}
./Restart.pl -h
\end{verbatim}

\section{What's next?}

Hopefully this section has guided you through installing the SWMF and
given you a basic knowledge of how to run it.  However it has probably
also convinced you that the SWMF is quite a complex tool and that there
are many more things for you to learn.  So, what next?

We suggest that you read all of chapter \ref{chapter:basics}, which
outlines the basic features of the SWMF as well as some things you
really must know in order to use the SWMF.  Once you have done this you
are ready to experiment.  Chapter \ref{chapter:examples} gives several 
examples which are intended to make you familiar with the use of the SWMF.
We suggest that you try them!

%\end{document}
