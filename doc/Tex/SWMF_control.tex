%  Copyright (C) 2002 Regents of the University of Michigan, portions used with permission 
%  For more information, see http://csem.engin.umich.edu/tools/swmf
\section{Execution and Coupling Control}

The control module of SWMF controls the execution and coupling of
components. The control module is controlled by the user through the
component layout file LAYOUT.in and the input parameter file PARAM.in.
Defining the most efficient layout, execution and coupling control
is not an obvious task. In the current version of SWMF the processor
layout of the components is static. This restriction is somewhat
mitigated by the possibility of restart, which allows to change
the processor layout from one run to another.

\subsection{Processor Layout: LAYOUT.in}

Within one run the layout is determined by the \#COMPONENTMAP
command in the LAYOUT.in file. The exact syntax of this file
is documented in the reference manual of the CON\_world class.
Here we provide several examples which will help to develop
a sense of using optimal layouts.  An optimal layout is one that 
maximizes the use of all processors and does not leave processors
with nothing do while waiting for other processors to finish their work.

First of all we have to define the processor rank:
it is a number ranging from 0 to $N-1$, where
$N$ is the total number of processors in the run. 
A component can run on a subset of the processors,
which is defined by the rank of the first (root) processor,
the rank of the last processor, and the stride. For
example if the root processor has rank 4, the last processor
has rank 8, and the stride is 2 than the component will
run on 3 processors with ranks 4, 6 and 8.

\subsubsection{One component}

In the simplest case a single component, say the Global
Magnetosphere (GM) is running. The LAYOUT.in file should be
the following
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
GM    0     999    1
#END
\end{verbatim}
If the SWMF is run on less than 999 processors (which is likely),
the rank of the last processor will be reduced to the maximum
rank of the processors, which is $N-1$ if the SWMF is running on $N$
processors.

\subsubsection{One serial and one parallel component}

When two components are used, their layouts may or may not overlap.
An example for overlapping the layouts of the GM and the
Inner Magnetosphere (IM) components is
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
GM    0     999    1
#END
\end{verbatim}
When the component layouts overlap, the two components can run
sequentially only. Since IM is using a single processor only
(because it is not a parallel code), all the other processors 
will be idling while IM is running. This can be rather inefficient,
especially if the CPU time required by IM is not negligible.
A more efficient execution can be achieved with a non-overlapping layout:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
GM    1     999    1
#END
\end{verbatim}
Note that this layout file will work for any number 
of processors from 2 and up.

\subsubsection{Two parallel components with different speeds}

It is not always possible, or even efficient to use non-overlapping
layouts. For example both the SC and IH components require a lot of memory,
but the IH component runs much faster (say 100 times faster) 
in terms of cpu time than the SC component (this is due to the 
larger cells and smaller wave speeds in IH).
If we tried to use concurrent execution on 101 processors,
SC should run on 100 and IH on 1 processors to get good load balancing.
However the IH component needs much more memory than available
on a single processor. It is therefore not possible to use a non-overlapping
layout for SC and IH on a reasonable number of processors.

Fortunately both the Solar Corona (SC) and Inner Heliosphere (IH)
components are modeled by \BATSRUS, which is a highly parallel code
with good scaling. The following layout can be optimal:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IH    0     999    1
SC    0     999    1
#END
\end{verbatim}
Although IH and SC will execute sequentially, they both
use all the available CPU-s, so no CPU is left waiting for the others.

\subsubsection{Two parallel components with similar speeds}

If two parallel components need about the same CPU time/real time
on the same number of processors, the optimal layout can be
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
GM    0     999    2
SC    1     999    2
#END
\end{verbatim}
Here GM is running on the processors with even rank,
while SC is running on the processors with odd ranks.
By using the processor stride, this layout works on
an arbitrary number of processors.

When more serial and parallel codes are executing together,
finding the optimal layout may not be trivial. 
It may take some experimentation to see which component
is running slower or faster, how much time is spent
on coupling two components, etc. It may be a good idea
to test the components separately or in smaller groups
to see how fast they can execute.

\subsubsection{A complex example with four components}

Here is an example with 4 components: the Ionospheric
Electrodynamics (IE) component can run on 2 processors and 
runs about 3 times faster than real time.
The serial Inner Magnetosphere (IM) component runs even faster,
on the other hand the coupling of GM and IM is rather
computationally expensive. The Upper Atmosphere (UA) component
can run on up to 32 processors, and it runs twice as fast
as real time. The Global Magnetosphere model (GM) needs
at least 32 processor to run faster than real time.
If we have a lot of CPU-s, we may simply create a non-overlapping
layout. Since GM has no restriction on the number of processors,
it can be the last component in the map
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
IE    0       1    1
UA    2      33    1
GM   34     999    1
#END
\end{verbatim}
This layout will be optimal in terms of speed for a large 
(more than 100) number of PE-s, and actually the maximum
speed is going to be limited by the components which do
not scale. On a more modest number of PE-s one can try
to overlap UA and GM:
\begin{verbatim}
ID   proc0 last stride
#COMPONENTMAP
IM    0       0    1
IE    1       2    1
UA    0      31    1
GM    3     999    1
#END
\end{verbatim}

\subsection{Steady State vs. Time Accurate Execution}

The SWMF can run both in time accurate (default) and
steady state mode. This sounds surprising first, 
since many of the components can run in time accurate 
mode only. Nevertheless, the SWMF can improve the convergence
towards a steady state by allowing the different components
to run at different speeds in terms of the physical time.
In \BATSRUS\ the same idea is used on a much smaller scale:
local time stepping allows each cell to converge towards
steady state as fast as possible, limited only by the local
numerical stability limit.

\subsubsection{Steady state session}

The steady state mode should be signaled with the 
\begin{verbatim}
#TIMEACCURATE
F                DoTimeAccurate
\end{verbatim}
command, usually placed somewhere at the beginning of the session.
Since the SWMF runs in time accurate mode by default,
this command is required in the first steady state session of the run.

When SWMF runs in steady state mode, the SWMF time is not
advanced and tSimulation usually keeps its default initial value,
which is zero. 
The components may or may not advance their own
internal times. The execution is controlled by the 
step number {\tt nStep}, which goes from its initial value 
to the final step allowed by the MaxIteration parameter
of the \#STOP command. The components are called at
the frequency defined by the \#CYCLE command. For example
\begin{verbatim}
#CYCLE
GM          NameComp
1           DnRun
#END

#CYCLE
IM          NameComp
2           DnRun
#END
\end{verbatim}
means that IM runs in every second time step of the SWMF.
By defining the DnRun parameter for all the components,
an arbitrary relative calling frequency can be obtained,
which can optimize the global convergence rate to steady state.
The default frequency is DnRun=1, i.e. the component is
run in every SWMF time step. 

The relative frequency can be important for numerical
stability too. When GM and IM are to be relaxed
to a steady state, the GM/BATSRUS code is running in 
local time stepping mode, while IM/RCM runs in time 
accurate mode internally. Since GM and IM are coupled
both ways, an instability can occur if both GM and IM
are run every time step, because the GM physical time
step is very small, and the MHD solution cannot relax
while being continuously pushed by the IM coupling.
This unphysical instability can be avoided by calling the
IM component less frequently.

The coupling frequencies should be set to be optimal
for reaching the steady state. If the components are
coupled too frequently, a lot of CPU time is spent
on the couplings. If they are coupled very infrequently,
the solution may become oscillatory instead of relaxing
into a (quasi-)steady state solution. For example
we used the
\begin{verbatim}
#COUPLE2
IMNameComp1
GMNameComp2
10                      DnCouple
-1.                     DtCouple
\end{verbatim}
command to couple the GM and IM components in both directions
in every 10-th SWMF iteration.
Note that according to the above \#CYCLE commands,
GM and IM do 10 and 5 steps between two couplings,
respectively. GM/BATSRUS uses 10 local time steps,
while IM advances by 5 five-second time steps.

Another example is the relaxation of SC and IH components.
Under usual conditions the solar wind is supersonic at the 
inner boundary of the IH component, thus the steady state SC
solution can be obtained first, and then IH can converge
to a steady state using the SC solution as the inner boundary 
condition. In this second stage SC does not need to run
(assuming that it has reached a good steady state solution),
it is only needed for providing the inner boundary condition for IH.
This can be achieved by
\begin{verbatim}
! No need to run SC too often, it is already in steady state
#CYCLE
SC               NameComp
1000             DnRun

! No need to couple SC too often
#COUPLE2
SC               NameSource
IH               NameTarget
1000             DnCouple
-1.0             DtCouple
\end{verbatim}
Since SC and IH are always coupled at the beginning of the session,
further couplings are not necessary.

\subsubsection{Time accurate session}

The SWMF runs in time accurate mode by default. The
\begin{verbatim}
#TIMEACCURATE
T                DoTimeAccurate
\end{verbatim}
command is only needed in a time accurate session following a 
steady state session.
In time accurate mode the components advance in time at
approximately the same rate. The component times are
only synchronized when necessary, i.e. when they
are coupled, when restart files are written, or 
at the end of session and execution. Since the time
steps (in terms of physical and/or CPU time) of the components can be 
vastly different, this minimal synchronization provides the 
most possibilities for efficient concurrent execution.

By default the component time steps are limited by the
time of couplings. This means that if GM can take 4 second
times steps, and it is coupled with IE every 5 seconds,
then every second GM time step will be truncated to 1 second.
There are two ways to avoid this. One is to choose the
coupling frequencies to be round multiples of the time steps
of the two components involved. This works well if both components
have fixed time steps and/or much smaller time steps than the 
coupling frequency.

In certain cases the efficiency can be improved with the
\#COUPLETIME command, which can allow a component to 
step through the coupling time. For example
\begin{verbatim}
#CYCLE
GM             NameComp
F              DoCoupleOnTime
\end{verbatim}
will allow the GM component to use 4 second time steps even
if it is coupled at every 5 seconds. Of course this will
make the data transferred during the coupling be 
first order accurate in time.

\subsection{Coupling order}

The default coupling order is usually optimal for accuracy
and consistency, but it may not be optimal for speed.
In particular, the IE/Ridley\_serial component solves
a Poisson type equation for the data received from the 
other components (GM and UA). For sake of accuracy
IE always uses the latest data received from the other
components. If GM, UA and IE are coupled
in the default order
\begin{verbatim}
#COUPLEORDER
4             nCouple	  
GM IE	      NameSourceTarget
UA IE	      NameSourceTarget
IE UA	      NameSourceTarget
IE GM	      NameSourceTarget
\end{verbatim}
and the to-IE and from-IE coupling times coincide, e.g.
\begin{verbatim}
#COUPLE2
GM            NameComp1
IE            NameComp2
10.0          DtCouple
-1            DnCouple

#COUPLE2
UA            NameComp1
IE            NameComp2
10.0          DtCouple
-1            DnCouple
\end{verbatim}
then GM and UA will have to wait until IE solves
the Poisson equation, because IE receives new data
and it is required to produce results immediately.
With the reversed coupling order
\begin{verbatim}
#COUPLEORDER
4             nCouple	  
IE UA	      NameSourceTarget
IE GM	      NameSourceTarget
GM IE	      NameSourceTarget
UA IE	      NameSourceTarget
\end{verbatim}
IE will provide the solution from the previously received data,
and it will have time to work on the new data while GM and UA
are working on their time steps. The reversed coupling order
allows the concurrent execution of IE with other components.
The temporal accuracy, on the other hand, will be somewhat worse.

To demonstrate that the coupling order is important, here
is a very {\bf inefficient} coupling order
\begin{verbatim}
#COUPLEORDER
4             nCouple	  
GM IE         NameSourceTarget
IE GM         NameSourceTarget
UA IE         NameSourceTarget
IE UA         NameSourceTarget
\end{verbatim}
in case the coupling times with GM and UA coincide (always at the beginning
of a the sessions).
With this coupling order, IE first receives information from GM,
then solves the Poisson equation and returns the information based
on the solution to GM while GM is waiting. Then IE receives extra
information from UA, solves the Poisson equation again, and sends
back information to UA, while UA is waiting. 

An alternative way to achieve concurrent execution is to
stagger the coupling times. For example the
\begin{verbatim}
#COUPLE2SHIFT
GM                 NameComp1
IE                 NameComp2
-1                 DnCouple
10.0               DtCouple
-1                 nNext12
0.0                tNext12
-1                 nNext21
5.0                tNext21
\end{verbatim}
will schedule a GM to IE coupling at 0, 10, 20, 30, \ldots seconds,
and the IE to GM coupling at 5, 15, 25, \ldots seconds.
This provides IE half the GM time to solve the Poisson equations.
If IE runs at least twice as fast as GM, this solution will
produce concurrent execution. The temporal accuracy is
somewhat better than in the reversed coupling case.
Note that GM and IE will be synchronized at 0, 5, 10, \ldots seconds,
which works best if the GM time step is an integer fraction of 5 seconds.
